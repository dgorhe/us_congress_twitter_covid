# Data transformation

## Congress Members Tweets
Some fields had the majority of their values missing and thus the feature itself is ignored. This was the case with the geolocation feature. We did not know ahead of time whether this would be useful and if it was more common, it might have allowed us to perform district level analyses for the US Congress members. Much of the cleaning was done when the data was being retrieved via the Twitter API. We used the new Tweepy python package and Python 3.9 to retrieve tweets from Twitter's V2 API endpoint. This new version of the endpoint provides features which were not utilized for this analysis. We chose the V2 endpoint to ensure that others can replicate our findings for as long as possible. The old API is not yet deprecated so the tweet retrieval is probably translatable to the V1.1 API although this has not been tested. The web scraping script can be found in the repository. The dataset of tweets is approximately 100 megabytes (Mb) and contains roughly 400,000 tweets across all US Congress Members.

### Tweet Cleaning Process
Most of the work was involved in cleaning the dataset. There were tweets returned which either did not have an id or did not contain any text. The presumption is that these tweets may have been posted and later retracted but the tweet ids remained linked to the congress member's account within Twitter's database. There was one tweet missing the Twitter handle which was eliminated. A more common issue was duplicate tweets. Understandably, members of the US Congress discuss matters via Twitter and there are often threads where members overlap. However, this only accounted for about 10,000 tweets removed out of the approximately 410,000 tweets initially scraped. We also removed emojis from tweets for easier processing and parsing of information. Emojis, as opposed to other unicode characters, caused issues in particular. Perhaps there is some insightful things to learn from US Congress members' use of emojis but that could be a topic for future studies.

### Appending Zero Shot Classification Labels
Zero-shot classification of tweet content was done via Google Colab Notebooks using the open source transformers package. This package included BERT and GPU inference was used to speed up labeling. Labels were stored separately and then appended to the tweet data after the fact.

## Google's Changes in Mobility Data
This data was provided directly by Google and can be found (here)[https://www.google.com/covid19/mobility/]. The US data lists relative changes in mobility at the state and county level. We will only use data from states because for most states multiple counties correspond to a single congressional district. The more important problem occurs when a single county is represented by multiple congressional districts (e.g Los Angeles County in California). If the many-to-one relationship only existed in one direction. We could theoretically, manually map congressional districts to counties or vice versa in a consistent manner. We could assume that counties with many districts have representatives who are equally influential but that assumption could sway our results significantly. For example, if we aggregate representatives from multiple districts into the county shared by those districts, we are assuming the representatives are acting independently which is a tenuous assumption at best. Representatives from the same county likely have similar constituents and goals so any uniformity from their tweet content would suggest a stronger relationship between mobility and tweets than would be otherwise. In the future, people could experiment with different weightings to make this corresponding more representative.

## Number of tweets and cases
As said, the data was provided by the two different datasets. One stating the cases and deaths (we'll call it t1 from now on) for each country in each day, the other is a webscraped version of the number of tweets where the top 1000 covid-related bigrams are present (we'll call it t2).
Looking at t1, we filter the table on the country and on the date. Indeed, we are going to pick only USA data, and data that starts from 4/1/2020 to 12/15/2020. Filtering on the USA is just a way to reduce our dataset, even though our twitter bigrams data is counting all bigrams in the world, it is clear that the majority of tweets come from the USA. 
For t2, we are also limiting our dates to the range 4/1/2020 to 12/15/2020. Furthermore, we are limiting the analysis to the top 100 bigrams instead of the 1000.We do this to sample the initial dataframe and have an easier approach when writing the code. We decided to get the related bigrams we wanted to include the bigram 'covid 19' which we believe is the most used bigrams when it comes to covid related words. Of course a similar analysis may be done on monograms and trigrams. The webscraping script can be found in the repository.
At the same time, we tried establishing a relationship between mobility, covid, and twitter data. To do this we used a part of USA mobility Data which was explained before
