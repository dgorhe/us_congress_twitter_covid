<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Missing values | US Congress - Tweets and COVID-19</title>
  <meta name="description" content="Chapter 4 Missing values | US Congress - Tweets and COVID-19" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Missing values | US Congress - Tweets and COVID-19" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Missing values | US Congress - Tweets and COVID-19" />
  
  
  

<meta name="author" content="Darvesh Gorhe, Thomas Holvoet, Nicolo Ricci" />


<meta name="date" content="2021-12-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-transformation.html"/>
<link rel="next" href="results.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/parallel-coordinates-2.1.8/parcoords.css" rel="stylesheet" />
<script src="libs/parcoords-binding-1.0.0/parcoords.js"></script>
<script src="libs/d3-5.16.0/d3.min.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">US Congress Tweet Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#congress-and-mobility"><i class="fa fa-check"></i><b>1.1</b> Congress and Mobility</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#congress-and-sentiment"><i class="fa fa-check"></i><b>1.2</b> Congress and Sentiment</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-sources.html"><a href="data-sources.html"><i class="fa fa-check"></i><b>2</b> Data sources</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-sources.html"><a href="data-sources.html#congress-members-tweets"><i class="fa fa-check"></i><b>2.1</b> Congress Members Tweets</a></li>
<li class="chapter" data-level="2.2" data-path="data-sources.html"><a href="data-sources.html#googles-changes-in-mobility-data"><i class="fa fa-check"></i><b>2.2</b> Google’s Changes in Mobility Data</a></li>
<li class="chapter" data-level="2.3" data-path="data-sources.html"><a href="data-sources.html#number-of-tweets-cases-and-mobility"><i class="fa fa-check"></i><b>2.3</b> Number of tweets cases and mobility</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-transformation.html"><a href="data-transformation.html"><i class="fa fa-check"></i><b>3</b> Data transformation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-transformation.html"><a href="data-transformation.html#congress-members-tweets-1"><i class="fa fa-check"></i><b>3.1</b> Congress Members Tweets</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="data-transformation.html"><a href="data-transformation.html#tweet-cleaning-process"><i class="fa fa-check"></i><b>3.1.1</b> Tweet Cleaning Process</a></li>
<li class="chapter" data-level="3.1.2" data-path="data-transformation.html"><a href="data-transformation.html#appending-zero-shot-classification-labels"><i class="fa fa-check"></i><b>3.1.2</b> Appending Zero Shot Classification Labels</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-transformation.html"><a href="data-transformation.html#googles-changes-in-mobility-data-1"><i class="fa fa-check"></i><b>3.2</b> Google’s Changes in Mobility Data</a></li>
<li class="chapter" data-level="3.3" data-path="data-transformation.html"><a href="data-transformation.html#number-of-tweets-cases-and-mobility-1"><i class="fa fa-check"></i><b>3.3</b> Number of tweets cases and mobility</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>4</b> Missing values</a>
<ul>
<li class="chapter" data-level="4.1" data-path="missing-values.html"><a href="missing-values.html#excluding-tweet-content-from-missing-values"><i class="fa fa-check"></i><b>4.1</b> Excluding Tweet Content from Missing Values</a></li>
<li class="chapter" data-level="4.2" data-path="missing-values.html"><a href="missing-values.html#geo"><i class="fa fa-check"></i><b>4.2</b> Geo</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="missing-values.html"><a href="missing-values.html#impact-of-missing-geo-values"><i class="fa fa-check"></i><b>4.2.1</b> Impact of Missing Geo Values</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="missing-values.html"><a href="missing-values.html#language"><i class="fa fa-check"></i><b>4.3</b> Language</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="missing-values.html"><a href="missing-values.html#impact-of-missing-lang-values"><i class="fa fa-check"></i><b>4.3.1</b> Impact of Missing Lang Values</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="missing-values.html"><a href="missing-values.html#other-columns"><i class="fa fa-check"></i><b>4.4</b> Other Columns</a></li>
<li class="chapter" data-level="4.5" data-path="missing-values.html"><a href="missing-values.html#number-of-tweets-cases-and-mobility-2"><i class="fa fa-check"></i><b>4.5</b> Number of tweets cases and mobility</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>5</b> Results</a></li>
<li class="chapter" data-level="6" data-path="section.html"><a href="section.html"><i class="fa fa-check"></i><b>6</b> <!-- Nicolo's Code START --></a></li>
<li class="chapter" data-level="7" data-path="r-we-filter-the-dataset-to-include-only-usa-this-is-because-we-are-assuming-that-the-majority-of-the-english-language-tweets-we-are-dealing-are-coming-from-usa.-in-data-we-are-storing-the-data-coming-from-httpswww.ecdc.europa.euenpublications-datadownload-todays-data-geographic-distribution-covid-19-cases-worldwide-which-is-recording-the-number-of-cases-and-deaths-ion-every-day-of-2020-by-country.-ignoring-testdata-datadatacountryterritorycodeusa-we-are-going-to-aggregate-our-values-in-ciao1-we-have-the-top-50-bigrams-in-tweets-for-each-day.-so-in-this-case-we-are-grouping-the-tweets-by-each-day-so-that-we-have-a-table-showing-yearmonth-day-and-the-total-number-of-covid-related-bigrams.-we-focused-on-bigrams-so-we-can-catch-the-combination-covid-19-among-others-ciao1_mod-aggregateciao1countsbylistciao1yearciao1monthciao1dayfunsum-we-are-going-to-change-the-columns-name-apparently-when-aggregating-we-are-losing-the-columns-names-colnamesciao1_mod1-year-colnamesciao1_mod2-month-colnamesciao1_mod3-day-colnamesc"><a href="r-we-filter-the-dataset-to-include-only-usa-this-is-because-we-are-assuming-that-the-majority-of-the-english-language-tweets-we-are-dealing-are-coming-from-usa.-in-data-we-are-storing-the-data-coming-from-httpswww.ecdc.europa.euenpublications-datadownload-todays-data-geographic-distribution-covid-19-cases-worldwide-which-is-recording-the-number-of-cases-and-deaths-ion-every-day-of-2020-by-country.-ignoring-testdata-datadatacountryterritorycodeusa-we-are-going-to-aggregate-our-values-in-ciao1-we-have-the-top-50-bigrams-in-tweets-for-each-day.-so-in-this-case-we-are-grouping-the-tweets-by-each-day-so-that-we-have-a-table-showing-yearmonth-day-and-the-total-number-of-covid-related-bigrams.-we-focused-on-bigrams-so-we-can-catch-the-combination-covid-19-among-others-ciao1_mod-aggregateciao1countsbylistciao1yearciao1monthciao1dayfunsum-we-are-going-to-change-the-columns-name-apparently-when-aggregating-we-are-losing-the-columns-names-colnamesciao1_mod1-year-colnamesciao1_mod2-month-colnamesciao1_mod3-day-colnamesc"><i class="fa fa-check"></i><b>7</b> <code>{r} # # we filter the dataset to include only USA, this is because we are assuming that the majority of the english language tweets we are dealing are coming from USA. # # In data we are storing the data coming from https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide # #which is recording the number of cases and deaths ion every day of 2020 by country. #  # # Ignoring # # testdata&lt;-data[data$countryterritoryCode=='USA',] #  # #We are going to aggregate our values, in ciao1 we have the top 50 bigrams in tweets for each day. So in this case we are grouping the tweets by each day, so that we have a table showing year,month, day and the total number of Covid-related bigrams. We focused on bigrams so we can catch the combination COVID 19 among others #  # ciao1_mod&lt;-aggregate(ciao1$counts,by=list(ciao1$year,ciao1$month,ciao1$day),FUN=sum) # #we are going to change the column's name (apparently when aggregating we are losing the columns names) # colnames(ciao1_mod)[1]&lt;-'year' # colnames(ciao1_mod)[2]&lt;-'month' # colnames(ciao1_mod)[3]&lt;-'day' # colnames(ciao1_mod)[4]&lt;-'tweet_counts' #  # #We are going to order our data by date, so we will be able to use a function to get rid of the dates and just use a common column called interval, so that we will be able to test our correlation between different spans of time # #We are going to order both cases and deaths, and tweets by their date # df3&lt;-testdata[order(testdata$year,testdata$month,testdata$day), ] # df3 %&gt;% map_df(rev) # ciao1_mod3&lt;-ciao1_mod[order(ciao1_mod$year,ciao1_mod$month,ciao1_mod$day),] # ciao1_mod3 %&gt;% map_df(rev) # #We are going to create a column interval, so that we can geet rid of the dates and ease up the merging of different tables and for now we set it to 0 # ciao1_mod3$interval=0 # df3$interval=0 # #Now we are going to align the two tables so to have similar periods of time for our analysis. For this purpose we are going to consider the period from 04/1/2021 to the day 12/14/2021 which is the last one in one of the two tables # #To do that we filter our rows in that specific period # df3&lt;-df3[df3$month&gt;=4 &amp; df3$year==2020,] # ciao1_mod3&lt;-ciao1_mod3[ciao1_mod3$month&gt;=4 &amp; ciao1_mod3$year==2020,] # #We find out that the cases and deaths df is only till 12/14/2020 but the tweets is to 12/30 so we are going to eliminate the last 16 rows in our tweets df # n&lt;-dim(ciao1_mod3)[1] # ciao1_mod3&lt;-ciao1_mod3[1:(n-16),] # #We obtained the two datasets cleaned and parsed, the next styep would be to implement a function to create intervals # # Instead of having dates, we are going to pass a parameter k and divide the days we had into intervals #  #</code></a></li>
<li class="chapter" data-level="8" data-path="r-we-decided-that-7-days-would-be-a-nice-interval-to-keep-it-means-we-are-going-to-conduct-our-correlation-weekly.-of-course-we-could-change-k-to-14-k-7-do_interval-functiondfinterval-we-are-going-to-start-numerating-it-from-1-cause-we-need-to-create-a-new-column-c1-for-row-in-1nrowdf-dfrowinterval--floorcinterval1-we-increase-our-counter-and-divide-by-the-interval-parameter-so-that-for-each-day-we-have-the-correspondant-interval-in-a-separeted-column-cc1-here-we-are-going-to-aggregate-the-cases-deaths-or-other-metrics-based-on-the-intervals-the-conditions-that-we-put-on-the-number-of-columns-is-used-to-differentiate-the-different-data-sets-that-we-are-going-to-pass.-if-ncoldf6-ifncoldf14-ncoldf10-we-are-in-the-cases-and-deaths-table-here-summing-the-variables-on-grouping-by-the-interval-df-aggregatecbinddfcasesdfdeathsbylistdfintervalfunsum-colnamesdf1-interval-colnamesdf2-cases-colnamesdf3-deaths-else-we-are-in-the-travel-table-here-df-aggregatecbinddfc16bylistdfintervalfunsum-colnamesdf1-interval-else-we-"><a href="r-we-decided-that-7-days-would-be-a-nice-interval-to-keep-it-means-we-are-going-to-conduct-our-correlation-weekly.-of-course-we-could-change-k-to-14-k-7-do_interval-functiondfinterval-we-are-going-to-start-numerating-it-from-1-cause-we-need-to-create-a-new-column-c1-for-row-in-1nrowdf-dfrowinterval--floorcinterval1-we-increase-our-counter-and-divide-by-the-interval-parameter-so-that-for-each-day-we-have-the-correspondant-interval-in-a-separeted-column-cc1-here-we-are-going-to-aggregate-the-cases-deaths-or-other-metrics-based-on-the-intervals-the-conditions-that-we-put-on-the-number-of-columns-is-used-to-differentiate-the-different-data-sets-that-we-are-going-to-pass.-if-ncoldf6-ifncoldf14-ncoldf10-we-are-in-the-cases-and-deaths-table-here-summing-the-variables-on-grouping-by-the-interval-df-aggregatecbinddfcasesdfdeathsbylistdfintervalfunsum-colnamesdf1-interval-colnamesdf2-cases-colnamesdf3-deaths-else-we-are-in-the-travel-table-here-df-aggregatecbinddfc16bylistdfintervalfunsum-colnamesdf1-interval-else-we-"><i class="fa fa-check"></i><b>8</b> <code>{r} # #We decided that 7 days would be a nice interval to keep, it means we are going to conduct our correlation weekly. Of course we could change k to 14 # k&lt;-7 # do_interval&lt;-function(df,interval){ #   # we are going to start numerating it from 1 cause we need to create a new column #   c=1 # for (row in 1:nrow(df)){ #     df[row,'interval']&lt;- floor(c/interval)+1 #    # #We increase our counter and divide by the interval parameter so that for each day we have the correspondant interval in a separeted column # c=c+1 # } #  #here we are going to aggregate the cases, deaths or other metrics based on the intervals #   #The conditions that we put on the number of columns is used to differentiate the different data sets that we are going to pass.  #   if (ncol(df)&gt;6){ #      #     if(ncol(df)&lt;14 &amp; ncol(df)!=10){ #     # We are in the cases and deaths table here #     #summing the variables on grouping by the interval # df&lt;-aggregate(cbind(df$cases,df$deaths),by=list(df$interval),FUN=sum) # colnames(df)[1]&lt;-'interval' # colnames(df)[2]&lt;-'cases' # colnames(df)[3]&lt;-'deaths' #     }else{ #   #we are in the travel table here #   df&lt;-aggregate(cbind(df[c(1:6)]),by=list(df$interval),FUN=sum) #   colnames(df)[1]&lt;-'interval' # } # }else{ #   #we are in the tweet counts table here #   df&lt;-aggregate(df$tweet_counts,by=list(df$interval),FUN=sum) #   colnames(df)[1]&lt;-'interval' #   colnames(df)[2]&lt;-'tweet_counts' #   } #   return(df) # } # df31&lt;-do_interval(df3,k) # df32&lt;-do_interval(ciao1_mod3,k) # # we are going to merge by our interval, so then we can calculate correlation and plot the scatterplot and the best fitting line. # df3_final&lt;-merge(df31,df32,by=c('interval')) # corr1&lt;-cor(df3_final$cases, df3_final$tweet_counts) # cortest1&lt;-cor.test(df3_final$cases, df3_final$tweet_counts) # corr2&lt;-cor(df3_final$deaths, df3_final$tweet_counts) # cortest2&lt;-cor.test(df3_final$deaths, df3_final$tweet_counts) # ggplot(df3_final, aes(x = deaths, y = tweet_counts)) + geom_point(size = 0.5, alpha = 0.3, col='black') + scale_x_continuous(labels = scales::comma)+geom_smooth(method=lm) # ggplot(df3_final, aes(x = cases, y = tweet_counts)) + geom_point(size = 0.5, alpha = 0.3, col='red') + scale_x_continuous(labels = scales::comma)+geom_smooth(method=lm) # #visualizing the time series of the cases  # ggplot(df3_final,aes(x=interval))+ geom_line(aes(y=cases),size = 0.5, alpha = 0.3, col='red') + geom_line(aes(y=tweet_counts),size = 0.5, alpha = 0.3, col='blue')+scale_x_continuous(labels = scales::comma)+xlab('week')+ylab('Cases and tweet counts') # # we decided to divide by the average so to normalize our results and have a sense of what the correlation is # # in this case, the y axis is not useful, but we just observe the patterns # ggplot(df3_final,aes(x=interval))+ geom_line(aes(y=deaths/mean(deaths)),size = 0.5, alpha = 0.3, col='red') + geom_line(aes(y=tweet_counts/mean(tweet_counts)),size = 0.5, alpha = 0.3, col='blue')+scale_x_continuous(labels = scales::comma)+xlab('week')+ylab('Proportions of deaths and tweet counts') #</code></a></li>
<li class="chapter" data-level="9" data-path="observations.-if-we-take-a-look-at-our-correlations-coefficient-we-can-see-two-different-stories.-when-it-comes-to-number-of-covid-related-tweets-in-relation-to-the-number-of-deaths-we-have-low-correlation-.-the-correlation-coefficient-is-this-case-is-0.21corr2-which-means-there-is-a-positive-correlation.-however-if-we-take-a-look-at-the-correlation-test-cortest2-we-can-see-that-the-pvalue-is-0.1951-hence-even-with-a-confidence-level-of-10-we-cannot-reject-the-null-hypothesis-that-the-correlation-happened-by-chance..html"><a href="observations.-if-we-take-a-look-at-our-correlations-coefficient-we-can-see-two-different-stories.-when-it-comes-to-number-of-covid-related-tweets-in-relation-to-the-number-of-deaths-we-have-low-correlation-.-the-correlation-coefficient-is-this-case-is-0.21corr2-which-means-there-is-a-positive-correlation.-however-if-we-take-a-look-at-the-correlation-test-cortest2-we-can-see-that-the-pvalue-is-0.1951-hence-even-with-a-confidence-level-of-10-we-cannot-reject-the-null-hypothesis-that-the-correlation-happened-by-chance..html"><i class="fa fa-check"></i><b>9</b> Observations. If we take a look at our correlations coefficient we can see two different stories. When it comes to number of covid-related tweets in relation to the number of deaths we have low correlation . The correlation coefficient is this case is 0.21(corr2), which means there is a positive correlation. However if we take a look at the correlation test cortest2 we can see that the pvalue is 0.1951, hence even with a confidence level of 10% we cannot reject the null hypothesis that the correlation happened by chance.</a></li>
<li class="chapter" data-level="10" data-path="on-the-other-hand-we-can-analyse-our-correlation-of-cases-and-number-of-tweets.in-corr1-we-have-stored-a-correlation-value-of--0.48-and-the-associated-pvalue-is-0.0025-which-means-we-can-reject-our-null-hypothesis-and-state-that-there-is-a-correlation-between-the-number-of-cases-and-number-of-tweets..html"><a href="on-the-other-hand-we-can-analyse-our-correlation-of-cases-and-number-of-tweets.in-corr1-we-have-stored-a-correlation-value-of--0.48-and-the-associated-pvalue-is-0.0025-which-means-we-can-reject-our-null-hypothesis-and-state-that-there-is-a-correlation-between-the-number-of-cases-and-number-of-tweets..html"><i class="fa fa-check"></i><b>10</b> On the other hand, we can analyse our correlation of cases and number of tweets.In corr1 we have stored a correlation value of -0.48 and the associated pvalue is 0.0025, which means we can reject our null hypothesis and state that there is a correlation between the number of cases and number of tweets.</a></li>
<li class="chapter" data-level="11" data-path="the-surprising-feature-is-that-the-correlation-is-negative-where-common-sense-may-suggest-there-would-be-a-positive-correlation.in-other-words-we-may-expect-higher-number-of-cases-could-trigger-a-higher-number-of-people-talking-about-it..html"><a href="the-surprising-feature-is-that-the-correlation-is-negative-where-common-sense-may-suggest-there-would-be-a-positive-correlation.in-other-words-we-may-expect-higher-number-of-cases-could-trigger-a-higher-number-of-people-talking-about-it..html"><i class="fa fa-check"></i><b>11</b> The surprising feature, is that the correlation is negative, where common sense may suggest there would be a positive correlation.In other words, we may expect, higher number of cases could trigger a higher number of people talking about it.</a></li>
<li class="chapter" data-level="12" data-path="the-truth-may-be-another-one.-we-suspect-that-the-real-correlation-may-be-linked-to-the-dates-itself.-as-we-can-see-in-the-interval-chart-we-have-that-in-the-first-5-weeks-start-date-412020-people-were-more-scared-and-more-prone-to-talk-about-it-then-as-the-virus-became-usual-among-our-lives-the-covid-tweets-chat-started-to-slowly-but-steadily-decline.-even-in-the-last-part-of-the-graph-when-cases-started-taking-off-exponentially-the-number-of-covid-related-tweets-continued-to-slow..html"><a href="the-truth-may-be-another-one.-we-suspect-that-the-real-correlation-may-be-linked-to-the-dates-itself.-as-we-can-see-in-the-interval-chart-we-have-that-in-the-first-5-weeks-start-date-412020-people-were-more-scared-and-more-prone-to-talk-about-it-then-as-the-virus-became-usual-among-our-lives-the-covid-tweets-chat-started-to-slowly-but-steadily-decline.-even-in-the-last-part-of-the-graph-when-cases-started-taking-off-exponentially-the-number-of-covid-related-tweets-continued-to-slow..html"><i class="fa fa-check"></i><b>12</b> The truth may be another one. We suspect that the real correlation may be linked to the dates itself. As we can see in the interval chart, we have that in the first 5 weeks (start date 4/1/2020), people were more scared and more prone to talk about it, then as the virus became usual among our lives, the covid tweets chat started to slowly but steadily decline. Even in the last part of the graph, when cases started taking off exponentially, the number of covid related tweets continued to slow.</a></li>
<li class="chapter" data-level="13" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>13</b> </a></li>
<li class="chapter" data-level="14" data-path="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc"><a href="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc"><i class="fa fa-check"></i><b>14</b> <code>{r} # # In movements we have the changing for percentage of transit of people in comparison to same period of time in 2019in the whole USA # dfmove&lt;-Movements # # We have to convert the dates that we have in a more suitable format so that we can use the comparison to other dfs # for (row in 1:nrow(dfmove)){ #    #     date&lt;-dfmove[row,'date'] #     arr&lt;-strsplit(date,split='-') #     dfmove[row,'year']=(arr[[1]][1]) #     dfmove[row,'month']=(arr[[1]][2]) #     dfmove[row,'day']=(arr[[1]][3]) #    # dfmove[row,'day']=strtoi(arr[[1]][3]) # } #  # dfmove$year&lt;-as.integer(dfmove$year) # dfmove$month&lt;-as.integer(dfmove$month) # dfmove$day&lt;-as.integer(dfmove$day) # dfmove&lt;-dfmove[dfmove$month&gt;=4 &amp; dfmove$year==2020,] # n&lt;-dim(dfmove)[1] # dfmove&lt;-dfmove[1:(n-18),] # # we are going to get the columns that we need, cleaning the data from useless columns # dfmove$interval&lt;-0 # dfmove&lt;-dfmove[c(10:19)] # #We order them by the date so we can use the function do_interval and compare the results # dfmove&lt;-dfmove[order(dfmove$year,dfmove$month,dfmove$day),] # dfmove %&gt;% map_df(rev) # dfmovem&lt;-do_interval(dfmove,k) # #To have the correct change over the period, we are going to divide the columns by the period #  # dfmovem[2:7]&lt;-dfmovem[,2:7]/k #  # df4final&lt;-merge(dfmovem,df32,by=c('interval')) # df4final1&lt;-merge(dfmovem,df31,by=c('interval')) # #We set a parameter so that we can easily change for our visualization # x1&lt;-df4final$retail_and_recreation_percent_change_from_baseline # corr41&lt;-cor(x1, df4final$tweet_counts) # cortest41&lt;-cor.test(x1, df4final$tweet_counts) # corr42&lt;-cor(x1, df4final1$cases) # cortest42&lt;-cor.test(x1, df4final1$cases) # ggplot(df4final, aes(x = x1, y = tweet_counts)) + geom_point(size = 0.5, alpha = 0.3, col='black') + scale_x_continuous(labels = scales::comma)+geom_smooth(method=lm) # ggplot(df4final1, aes(x=df4final1$cases, y = x1)) + geom_point(size = 0.5, alpha = 0.3, col='black') + scale_x_continuous(labels = scales::comma)+geom_smooth(method=lm) # #create a ridgepllot with the different  # #pivoted&lt;-pivot_longer(df4final,cols = #c(df4final$retail_and_recreation_percent_change_from_baseline,df4final$grocery_and_pharmacy_percent_change_from_basel#ine,df4final$parks_percent_change_from_baseline,df4final$transit_stations_percent_change_from_baseline,df4final$workp#laces_percent_change_from_baseline,df4final$residential_percent_change_from_baseline),names_to = 'type',values_to = #'vals')</code></a>
<ul>
<li class="chapter" data-level="14.1" data-path="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc"><a href="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc#covid-vs.-mobility"><i class="fa fa-check"></i><b>14.1</b> COVID vs. Mobility</a></li>
<li class="chapter" data-level="14.2" data-path="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc"><a href="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc#analyzing-the-policy-of-representatives-of-congress-through-their-tweets"><i class="fa fa-check"></i><b>14.2</b> Analyzing the policy of representatives of Congress through their tweets</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc"><a href="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc#extracting-the-data"><i class="fa fa-check"></i><b>14.2.1</b> Extracting the data</a></li>
<li class="chapter" data-level="14.2.2" data-path="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc"><a href="r-in-movements-we-have-the-changing-for-percentage-of-transit-of-people-in-comparison-to-same-period-of-time-in-2019in-the-whole-usa-dfmove-movements-we-have-to-convert-the-dates-that-we-have-in-a-more-suitable-format-so-that-we-can-use-the-comparison-to-other-dfs-for-row-in-1nrowdfmove-date-dfmoverowdate-arr-strsplitdatesplit--dfmoverowyeararr11-dfmoverowmontharr12-dfmoverowdayarr13-dfmoverowdaystrtoiarr13-dfmoveyear-as.integerdfmoveyear-dfmovemonth-as.integerdfmovemonth-dfmoveday-as.integerdfmoveday-dfmove-dfmovedfmovemonth4-dfmoveyear2020-n-dimdfmove1-dfmove-dfmove1n-18-we-are-going-to-get-the-columns-that-we-need-cleaning-the-data-from-useless-columns-dfmoveinterval-0-dfmove-dfmovec1019-we-order-them-by-the-date-so-we-can-use-the-function-do_interval-and-compare-the-results-dfmove-dfmoveorderdfmoveyeardfmovemonthdfmoveday-dfmove-map_dfrev-dfmovem-do_intervaldfmovek-to-have-the-correct-change-over-the-period-we-are-going-to-divide-the-columns-by-the-period-dfmovem27-dfmovem27k-df4final-mergedfmovemdf32byc#patterns-in-policy"><i class="fa fa-check"></i><b>14.2.2</b> Patterns in policy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="interactive-component.html"><a href="interactive-component.html"><i class="fa fa-check"></i><b>15</b> Interactive component</a></li>
<li class="chapter" data-level="16" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>16</b> Conclusion</a>
<ul>
<li class="chapter" data-level="16.1" data-path="conclusion.html"><a href="conclusion.html#covid-19-vs-mobility-for-us-congress-members"><i class="fa fa-check"></i><b>16.1</b> COVID-19 vs Mobility for US Congress Members</a></li>
<li class="chapter" data-level="16.2" data-path="conclusion.html"><a href="conclusion.html#policy-related-tweets"><i class="fa fa-check"></i><b>16.2</b> Policy Related Tweets</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">US Congress - Tweets and COVID-19</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="missing-values" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Missing values</h1>
<p><img src="finalproj_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div id="excluding-tweet-content-from-missing-values" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Excluding Tweet Content from Missing Values</h2>
<p>We excluded the column <code>tweet_content</code> because it will naturally have very different values. If we had things like sentiments, annotations, counts of handles, etc that information could be examined for missing values. Since we’re extracting tweets by their id, it is extremely unlikely that a row would not contain the tweet content. As such, there isn’t much to gain by checking for missing values in the <code>tweet_content</code> column. Given the diversity of tweets and congresspeople, it’s likely that any one word, mention, etc is quite sparse across all tweets across all congress members and perhaps even within a single congress member.</p>
</div>
<div id="geo" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Geo</h2>
<p>Prior to extracting the tweets, we didn’t know how many would be geo-tagged. It’s apparent that the majority of tweets are not geo-tagged. The lack of geo tags do not affect the content of our analyses very much since we can make strong assumptions about the location of each senator or representative. We could group congress people by their specific district and constituency, but due to things like gerrymandering this could be an issue. Moreover, this would require us to associate a senator with all districts or no districts.</p>
<div id="impact-of-missing-geo-values" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Impact of Missing Geo Values</h3>
<p>The lack of geo-tagged tweets is not a huge issue for our analyses although it would be helpful to confirm assumptions. For example, if we had geo-tagged tweets, we could compare tweets posted outside of the congress member’s state versus tweets posted within a congress member’s state. But alas, we cannot have everything we desire.</p>
</div>
</div>
<div id="language" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Language</h2>
<p>Although much less frequent, the lang column contains a few missing values. When extracting data, Twitter labels those tweets whose language it cannot identify as “und” or undetermined. Typically, these tweets are labeled as such because they only contain URLs, hashtags, emojis, or some combination thereof. Undetermined tweets comprise the plurality of the non-English labled tweets which is interesting. There are even more undetermined tweets than Spanish tweets.</p>
<p><img src="finalproj_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>In total 376 people in Congress had tweets labeled as und. however, of those 376, are there any members whose tweets have a significantly higher percentage of tweets labeled as und? There are 13 US Congress members with 5% or more of their tweets labeled und for the language.</p>
<p><img src="finalproj_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Do these members of congress contribute a large portion of total tweets through their tweets labeled as und? Those with the highest proportion of tweets labeled und also tend to have the largest number of tweets labeled und.</p>
<p><img src="finalproj_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div id="impact-of-missing-lang-values" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Impact of Missing Lang Values</h3>
<p>The und values for lang are not necessarily a detriment to our overall analysis especially since they comprise an small proportion of overall tweets. Moreover, other than a few US Congress members, und tweets are not very common. The vast majority have less than 5% of their tweets labeled as und. Additionally, even with these tweets labeled as und, we may still be able to extract meaningful insights from the content of the tweets. The tweets themselves could contain words like COVID, COVID-19, the pandemic, etc. These would still help us understand the relationships between tweets and COVID.</p>
</div>
</div>
<div id="other-columns" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Other Columns</h2>
<p>There are no missing values from the other columns. This is mainly a result of the way the data was extracted. We had known Twitter users in mind and we extracted all their tweets within a specified time domain. Every tweet that could be retrieved has a corresponding identifying number (i.e. tweet_id field is non-empty). This isn’t a problem since it gives us much more information to work with. There are entire fields dedicated to analyzing subsets of linguistic content. From this data we can look at a multitude of features that could be useful to relate to COVID. For example, we can look at the frequency of the words COVID and COVID-19 over time, across congress members, and consequently by region. We have almost 500,000 tweets to analyze so even the simplest features will be useful.</p>
</div>
<div id="number-of-tweets-cases-and-mobility-2" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Number of tweets cases and mobility</h2>
<p>We did not encounter any problem with the missing values, indeed, the problem was the opposite, too much data. We were lucky enough to find very high quality data online.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-transformation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dgorhe/us_congress_twitter_covid/edit/main/04-missing.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/dgorhe/us_congress_twitter_covid/blob/main/04-missing.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
