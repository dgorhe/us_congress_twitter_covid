[["index.html", "US Congress - Tweets and COVID-19 Chapter 1 Introduction 1.1 Congress and Mobility 1.2 Congress and Sentiment", " US Congress - Tweets and COVID-19 Darvesh Gorhe, Thomas Holvoet, Nicolo Ricci 2021-12-14 Chapter 1 Introduction Data sources about COVID-19 abound and many have been visualized thoroughly. As such, to visually explore something new about COVID-19 is a challenge. However, a good place to start is by asking the question: what can we look at now that we couldn’t look at previously in regards to COVID-19? That is to say, there is data which is retroactively useful to visualize that did not lend itself to daily, weekly, and monthly updates at the height of the pandemic. Some of these data sources are ones that have been used for those daily, weekly, and monthly updates. We do our best to augment those sources to provide new insights. 1.1 Congress and Mobility An interesting proxy that we have not found to be thoroughly studied is the relationship between public tweets by members of the United States Congress and mobility data. Pre-vaccine, mobility is a useful proxy for the spread of COVID-19 at least in a directional sense. Ignoring things like mask compliance, we can gain some coarse insights into the relationship of our constitutional representation and people’s actual decision making. These insights can be determined through Google’s publicly available changes in mobility datasets for each state and county therein. Since most congressional districts contain more than one county, we aggregate data by states rather than a per district level. We want to answer questions relating to the following: how do US Congress members’ tweets correspond with changes in mobility? The null hypothesis in our case is that there is no relationship. The existence of a relationship would not necessarily imply a causal relationship. Nonetheless, it would be interesting to find two proxies for COVID-19 themselves having a relationship between one another. The changes in mobility data are not very granular for privacy reasons, so we will primarily investigate different aspects of the tweets. The tweet content itself has an incredibly high dimensionality, but even features like the number of tweets will be useful to understand and “weight” relationships appropriately. Not all members of the US Congress tweet frequently, and some do not tweet at all. It could be the case that more tweets dilute the effect of any particular representative. On the other hand, members who tweet frequently might actually be more vocal and thereby more influential toward their constituents. This is just one example of how nuanced Twitter data can be. 1.2 Congress and Sentiment A related concept is the sentiment of tweets from members of the US Congress. While the general public may have any amount of information from varying sources, US Congress members have access to confidential information which may, directly or indirectly, influence how they speak on public platforms like Twitter. We aim to look things adjacent to COVID-19 which may or may not be captured in the Congress and Mobility Section. We are classifying things with a coarse method even if it is considered state-of-the-art. Things like mask compliance, herd immunity, and social distancing are very obviously concepts related to COVID-19 but they may not be recognized as such when labeling with zero-shot classification. "],["data-sources.html", "Chapter 2 Data sources 2.1 Congress Members Tweets 2.2 Google’s Changes in Mobility Data 2.3 Number of tweets cases and mobility", " Chapter 2 Data sources https://github.com/unitedstates/congress-legislators/ https://github.com/thepanacealab/covid19_twitter https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide 2.1 Congress Members Tweets Since there are a lot of tweets and we don’t have a developer account such that we could retrieve millions of tweets successfully, we opted to get the most basic information for each tweet. We retrieved the user’s handle (i.e. username), the content of the tweet, geolocation data if any, the tweet id, and the language in which the tweet was written as determined by Twitter. If a language cannot be identified it is labeled as “und”. This label is a bit misleading at times, but this issue is discussed further in the Missing Data section of this report. We limited our search to tweets from handles according to (this)[https://github.com/unitedstates/congress-legislators/] dataset of Twitter handles for each congress member. We also limited our search to tweets from 12:00 AM UTC 2020 to 11:59 PM UTC 2020. These seemed like reasonable bounds since COVID-19 was particularly virulent in the year 2020 and the vaccine was not widely available before 2021 either. There were other features we could get from Twitter such as an entities object and sentiment analysis. We felt that the utility of these sentiments was not worth the extra time it would take to retrieve and parse through those fields. Moreover, we can still choose NLP models differing from those used by Twitter to extract information from tweet content. Thus extracting this information at the outset was not necessary, but comparing twitter sentiments and entities to those from other models could be useful in future work. 2.2 Google’s Changes in Mobility Data There is not much we can change or alter about this dataset and it is quite robust. The gist of the dataset is that Google used it’s geolocation data from its various users and services to determine relative changes in mobility between categories of places. These categories include: Retail &amp; Recreation Grocery &amp; Pharmacy, Parks, Transit Stations, Workplaces, and Residential. Obviously, this is not a comprehensive list of all categories of places where people may congregate, but it is nonetheless a representative sample of places where we would expect a high volume of human traffic. Other places which are not included which could be useful to inquire about are large conventions. Even if they are less frequent, the high concentration of people and relative changes therein may have an outsized impact on COVID-19. This could be a useful follow-up to this project but separate data may need to be sourced or scraped from the web. 2.3 Number of tweets cases and mobility The data sources we used for this part were the ecdc dataset about the COVID-19 cases and deaths during 2020 and the twitter data. In the first case, we filtered the table just to include the USA data, so that we could have a table diplaying date, number of cases, and deaths for each day. To better compare our data, we just focused on the prevaccine covid situation, starting from the 4/1/2020 till around 12/15/2020. For the twitter data, we had to webscrape the parts that we needed. The original dataset had the top 1000 bigrams in terms of occurrence for each day . To ease up our processes we decided to include only the top 50 most used covid-related bigrams.Furthermore, we also used the mobility data for general USA overview, that was webscraped previously. "],["data-transformation.html", "Chapter 3 Data transformation 3.1 Congress Members Tweets 3.2 Google’s Changes in Mobility Data 3.3 Number of tweets cases and mobility", " Chapter 3 Data transformation 3.1 Congress Members Tweets Some fields had the majority of their values missing and thus the feature itself is ignored. This was the case with the geolocation feature. We did not know ahead of time whether this would be useful and if it was more common, it might have allowed us to perform district level analyses for the US Congress members. Much of the cleaning was done when the data was being retrieved via the Twitter API. We used the new Tweepy python package and Python 3.9 to retrieve tweets from Twitter’s V2 API endpoint. This new version of the endpoint provides features which were not utilized for this analysis. We chose the V2 endpoint to ensure that others can replicate our findings for as long as possible. The old API is not yet deprecated so the tweet retrieval is probably translatable to the V1.1 API although this has not been tested. The web scraping script can be found in the repository. The dataset of tweets is approximately 100 megabytes (Mb) and contains roughly 400,000 tweets across all US Congress Members. 3.1.1 Tweet Cleaning Process Most of the work was involved in cleaning the dataset. There were tweets returned which either did not have an id or did not contain any text. The presumption is that these tweets may have been posted and later retracted but the tweet ids remained linked to the congress member’s account within Twitter’s database. There was one tweet missing the Twitter handle which was eliminated. A more common issue was duplicate tweets. Understandably, members of the US Congress discuss matters via Twitter and there are often threads where members overlap. However, this only accounted for about 10,000 tweets removed out of the approximately 410,000 tweets initially scraped. We also removed emojis from tweets for easier processing and parsing of information. Emojis, as opposed to other unicode characters, caused issues in particular. Perhaps there is some insightful things to learn from US Congress members’ use of emojis but that could be a topic for future studies. 3.1.2 Appending Zero Shot Classification Labels Zero-shot classification of tweet content was done via Google Colab Notebooks using the open source transformers package. This package included BERT and GPU inference was used to speed up labeling. Labels were stored separately and then appended to the tweet data after the fact. 3.2 Google’s Changes in Mobility Data This data was provided directly by Google and can be found (here)[https://www.google.com/covid19/mobility/]. The US data lists relative changes in mobility at the state and county level. We will only use data from states because for most states multiple counties correspond to a single congressional district. The more important problem occurs when a single county is represented by multiple congressional districts (e.g Los Angeles County in California). If the many-to-one relationship only existed in one direction. We could theoretically, manually map congressional districts to counties or vice versa in a consistent manner. We could assume that counties with many districts have representatives who are equally influential but that assumption could sway our results significantly. For example, if we aggregate representatives from multiple districts into the county shared by those districts, we are assuming the representatives are acting independently which is a tenuous assumption at best. Representatives from the same county likely have similar constituents and goals so any uniformity from their tweet content would suggest a stronger relationship between mobility and tweets than would be otherwise. In the future, people could experiment with different weightings to make this corresponding more representative. 3.3 Number of tweets cases and mobility As said, the data was provided by the three different datasets. One stating the cases and deaths (we’ll call it t1 from now on) for each country in each day, the other is a webscraped version of the number of tweets where the top 1000 covid-related bigrams are present (we’ll call it t2). Looking at t1, we filter the table on the country and on the date. Indeed, we are going to pick only USA data, and data that starts from 4/1/2020 to 12/15/2020. Filtering on the USA is just a way to reduce our dataset, even though our twitter bigrams data is counting all bigrams in the world, it is clear that the majority of tweets come from the USA. For t2, we are also limiting our dates to the range 4/1/2020 to 12/15/2020. Furthermore, we are limiting the analysis to the top 50 bigrams instead of the 1000.We do this to sample the initial dataframe and have an easier approach when writing the code. We decided to get the related bigrams we wanted to include the bigram ‘covid 19’ which we believe is the most used bigrams when it comes to covid related words. Of course a similar analysis may be done on monograms and trigrams. The webscraping script can be found in the repository. At the same time, we tried establishing a relationship between mobility, covid, and twitter data. To do this we used a part of USA mobility Data which was explained before "],["missing-values.html", "Chapter 4 Missing values 4.1 Excluding Tweet Content from Missing Values 4.2 Geo 4.3 Language 4.4 Other Columns 4.5 Number of tweets cases and mobility", " Chapter 4 Missing values 4.1 Excluding Tweet Content from Missing Values We excluded the column tweet_content because it will naturally have very different values. If we had things like sentiments, annotations, counts of handles, etc that information could be examined for missing values. Since we’re extracting tweets by their id, it is extremely unlikely that a row would not contain the tweet content. As such, there isn’t much to gain by checking for missing values in the tweet_content column. Given the diversity of tweets and congresspeople, it’s likely that any one word, mention, etc is quite sparse across all tweets across all congress members and perhaps even within a single congress member. 4.2 Geo Prior to extracting the tweets, we didn’t know how many would be geo-tagged. It’s apparent that the majority of tweets are not geo-tagged. The lack of geo tags do not affect the content of our analyses very much since we can make strong assumptions about the location of each senator or representative. We could group congress people by their specific district and constituency, but due to things like gerrymandering this could be an issue. Moreover, this would require us to associate a senator with all districts or no districts. 4.2.1 Impact of Missing Geo Values The lack of geo-tagged tweets is not a huge issue for our analyses although it would be helpful to confirm assumptions. For example, if we had geo-tagged tweets, we could compare tweets posted outside of the congress member’s state versus tweets posted within a congress member’s state. But alas, we cannot have everything we desire. 4.3 Language Although much less frequent, the lang column contains a few missing values. When extracting data, Twitter labels those tweets whose language it cannot identify as “und” or undetermined. Typically, these tweets are labeled as such because they only contain URLs, hashtags, emojis, or some combination thereof. Undetermined tweets comprise the plurality of the non-English labled tweets which is interesting. There are even more undetermined tweets than Spanish tweets. In total 376 people in Congress had tweets labeled as und. however, of those 376, are there any members whose tweets have a significantly higher percentage of tweets labeled as und? There are 13 US Congress members with 5% or more of their tweets labeled und for the language. Do these members of congress contribute a large portion of total tweets through their tweets labeled as und? Those with the highest proportion of tweets labeled und also tend to have the largest number of tweets labeled und. 4.3.1 Impact of Missing Lang Values The und values for lang are not necessarily a detriment to our overall analysis especially since they comprise an small proportion of overall tweets. Moreover, other than a few US Congress members, und tweets are not very common. The vast majority have less than 5% of their tweets labeled as und. Additionally, even with these tweets labeled as und, we may still be able to extract meaningful insights from the content of the tweets. The tweets themselves could contain words like COVID, COVID-19, the pandemic, etc. These would still help us understand the relationships between tweets and COVID. 4.4 Other Columns There are no missing values from the other columns. This is mainly a result of the way the data was extracted. We had known Twitter users in mind and we extracted all their tweets within a specified time domain. Every tweet that could be retrieved has a corresponding identifying number (i.e. tweet_id field is non-empty). This isn’t a problem since it gives us much more information to work with. There are entire fields dedicated to analyzing subsets of linguistic content. From this data we can look at a multitude of features that could be useful to relate to COVID. For example, we can look at the frequency of the words COVID and COVID-19 over time, across congress members, and consequently by region. We have almost 500,000 tweets to analyze so even the simplest features will be useful. 4.5 Number of tweets cases and mobility We did not encounter any problem with the missing values, indeed, the problem was the opposite, too much data. We were lucky enough to find very high quality data online. "],["results.html", "Chapter 5 Results", " Chapter 5 Results "],["section.html", "Chapter 6 ", " Chapter 6 "],["r-we-filter-the-dataset-to-include-only-usa-this-is-because-we-are-assuming-that-the-majority-of-the-english-language-tweets-we-are-dealing-are-coming-from-usa.-in-data-we-are-storing-the-data-coming-from-httpswww.ecdc.europa.euenpublications-datadownload-todays-data-geographic-distribution-covid-19-cases-worldwide-which-is-recording-the-number-of-cases-and-deaths-ion-every-day-of-2020-by-country.-ignoring-testdata-datadatacountryterritorycodeusa-we-are-going-to-aggregate-our-values-in-ciao1-we-have-the-top-50-bigrams-in-tweets-for-each-day.-so-in-this-case-we-are-grouping-the-tweets-by-each-day-so-that-we-have-a-table-showing-yearmonth-day-and-the-total-number-of-covid-related-bigrams.-we-focused-on-bigrams-so-we-can-catch-the-combination-covid-19-among-others-ciao1_mod-aggregateciao1countsbylistciao1yearciao1monthciao1dayfunsum-we-are-going-to-change-the-columns-name-apparently-when-aggregating-we-are-losing-the-columns-names-colnamesciao1_mod1-year-colnamesciao1_mod2-month-colnamesciao1_mod3-day-colnamesc", "Chapter 7 {r} # # we filter the dataset to include only USA, this is because we are assuming that the majority of the english language tweets we are dealing are coming from USA. # # In data we are storing the data coming from https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide # #which is recording the number of cases and deaths ion every day of 2020 by country. # # # Ignoring # # testdata&lt;-data[data$countryterritoryCode=='USA',] # # #We are going to aggregate our values, in ciao1 we have the top 50 bigrams in tweets for each day. So in this case we are grouping the tweets by each day, so that we have a table showing year,month, day and the total number of Covid-related bigrams. We focused on bigrams so we can catch the combination COVID 19 among others # # ciao1_mod&lt;-aggregate(ciao1$counts,by=list(ciao1$year,ciao1$month,ciao1$day),FUN=sum) # #we are going to change the column's name (apparently when aggregating we are losing the columns names) # colnames(ciao1_mod)[1]&lt;-'year' # colnames(ciao1_mod)[2]&lt;-'month' # colnames(ciao1_mod)[3]&lt;-'day' # colnames(ciao1_mod)[4]&lt;-'tweet_counts' # # #We are going to order our data by date, so we will be able to use a function to get rid of the dates and just use a common column called interval, so that we will be able to test our correlation between different spans of time # #We are going to order both cases and deaths, and tweets by their date # df3&lt;-testdata[order(testdata$year,testdata$month,testdata$day), ] # df3 %&gt;% map_df(rev) # ciao1_mod3&lt;-ciao1_mod[order(ciao1_mod$year,ciao1_mod$month,ciao1_mod$day),] # ciao1_mod3 %&gt;% map_df(rev) # #We are going to create a column interval, so that we can geet rid of the dates and ease up the merging of different tables and for now we set it to 0 # ciao1_mod3$interval=0 # df3$interval=0 # #Now we are going to align the two tables so to have similar periods of time for our analysis. For this purpose we are going to consider the period from 04/1/2021 to the day 12/14/2021 which is the last one in one of the two tables # #To do that we filter our rows in that specific period # df3&lt;-df3[df3$month&gt;=4 &amp; df3$year==2020,] # ciao1_mod3&lt;-ciao1_mod3[ciao1_mod3$month&gt;=4 &amp; ciao1_mod3$year==2020,] # #We find out that the cases and deaths df is only till 12/14/2020 but the tweets is to 12/30 so we are going to eliminate the last 16 rows in our tweets df # n&lt;-dim(ciao1_mod3)[1] # ciao1_mod3&lt;-ciao1_mod3[1:(n-16),] # #We obtained the two datasets cleaned and parsed, the next styep would be to implement a function to create intervals # # Instead of having dates, we are going to pass a parameter k and divide the days we had into intervals # #", " Chapter 7 {r} # # we filter the dataset to include only USA, this is because we are assuming that the majority of the english language tweets we are dealing are coming from USA. # # In data we are storing the data coming from https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide # #which is recording the number of cases and deaths ion every day of 2020 by country. # # # Ignoring # # testdata&lt;-data[data$countryterritoryCode=='USA',] # # #We are going to aggregate our values, in ciao1 we have the top 50 bigrams in tweets for each day. So in this case we are grouping the tweets by each day, so that we have a table showing year,month, day and the total number of Covid-related bigrams. We focused on bigrams so we can catch the combination COVID 19 among others # # ciao1_mod&lt;-aggregate(ciao1$counts,by=list(ciao1$year,ciao1$month,ciao1$day),FUN=sum) # #we are going to change the column's name (apparently when aggregating we are losing the columns names) # colnames(ciao1_mod)[1]&lt;-'year' # colnames(ciao1_mod)[2]&lt;-'month' # colnames(ciao1_mod)[3]&lt;-'day' # colnames(ciao1_mod)[4]&lt;-'tweet_counts' # # #We are going to order our data by date, so we will be able to use a function to get rid of the dates and just use a common column called interval, so that we will be able to test our correlation between different spans of time # #We are going to order both cases and deaths, and tweets by their date # df3&lt;-testdata[order(testdata$year,testdata$month,testdata$day), ] # df3 %&gt;% map_df(rev) # ciao1_mod3&lt;-ciao1_mod[order(ciao1_mod$year,ciao1_mod$month,ciao1_mod$day),] # ciao1_mod3 %&gt;% map_df(rev) # #We are going to create a column interval, so that we can geet rid of the dates and ease up the merging of different tables and for now we set it to 0 # ciao1_mod3$interval=0 # df3$interval=0 # #Now we are going to align the two tables so to have similar periods of time for our analysis. For this purpose we are going to consider the period from 04/1/2021 to the day 12/14/2021 which is the last one in one of the two tables # #To do that we filter our rows in that specific period # df3&lt;-df3[df3$month&gt;=4 &amp; df3$year==2020,] # ciao1_mod3&lt;-ciao1_mod3[ciao1_mod3$month&gt;=4 &amp; ciao1_mod3$year==2020,] # #We find out that the cases and deaths df is only till 12/14/2020 but the tweets is to 12/30 so we are going to eliminate the last 16 rows in our tweets df # n&lt;-dim(ciao1_mod3)[1] # ciao1_mod3&lt;-ciao1_mod3[1:(n-16),] # #We obtained the two datasets cleaned and parsed, the next styep would be to implement a function to create intervals # # Instead of having dates, we are going to pass a parameter k and divide the days we had into intervals # # "]]
